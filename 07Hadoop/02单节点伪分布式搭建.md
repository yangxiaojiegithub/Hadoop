# Hadoop单节点伪分布式搭建

## 节点规划

|        |             |
| ------ | ----------- |
| node01 | NN，DN，SNN |

## 操作系统环境信息准备

1. 检查hosts文件, node01映射正确

   ```shell
   [root@node01 stanlong]# cat /etc/hosts
   127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
   ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
   192.168.235.11 node01
   192.168.235.12 node02
   192.168.235.13 node03
   192.168.235.14 node04
   [root@node01 stanlong]# 
   ```

2. 检查hostname

   ```shell
   [root@node01 stanlong]# hostname
   node01
   [root@node01 stanlong]# cat /etc/sysconfig/network
   # Created by anaconda
   HOSTNAME=node01
   [root@node01 stanlong]# 
   ```

3. 安装jdk并配置环境变量（参考04软件安装）

4. 配置单机免密钥登录（参考 02Linux基本命令/免密钥配置.md）

## Hadoop部署

### 准备hadoop-2.9.2.tar.gz

```shell
[root@node01 ~]# ll
-rw-r--r--   1 root  root  348326890 Jan 12 21:49 hadoop-2.9.2.tar.gz
```

### 解压

```shell
# 解压到指定目录 /opt/stanlong/hadoop-single/
[root@node01 ~]# tar -zxf  hadoop-2.9.2.tar.gz -C /opt/stanlong/hadoop-single/  
[root@node01 hadoop-single]# ll
total 0
drwxr-xr-x 9 1001 1002 149 Jul 21 16:27 hadoop-2.9.2
```

### 配置 hadoop环境变量

```shell
[root@node01 bin]# vi /etc/profile

在文件末尾添加HADOOP环境变量
export HADOOP_HOME=/opt/stanlong/hadoop-single/hadoop-2.9.2 # HADOOP单节点环境变量
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

使环境变量生效
[root@node01 bin]# source /etc/profile

# 输入ha能看到命令提示说明环境变量配置成功
[root@node01 bin]# ha
hadoop             hadoop.cmd         hadoop-daemon.sh   hadoop-daemons.sh  halt               hardlink           hash      
```

### java环境变量二次配置

```shell
[root@node01 hadoop]# pwd
/opt/stanlong/hadoop-single/hadoop-2.9.2/etc/hadoop  # hadoop 文件的配置目录
[root@node01 hadoop]# vi hadoop-env.sh 
  24 # The java implementation to use.
  25 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi mapred-env.sh 
  16 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi yarn-env.sh
  23 export JAVA_HOME=/usr/java/jdk1.8.0_65
```

### 编辑 core-site.xml(NN)

```shell
[root@node01 hadoop]# vi core-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!-- 规划了namenode在哪启动 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
    <!-- 配置NN数据存放路径,目录必须为空 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/data/hadoop/local</value>
    </property>
</configuration>
```

### 编辑 hdfs-site.xml(NN)

```shell
[root@node01 hadoop]# vi hdfs-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!-- 规划了secondaryName在哪启动 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:50090</value>
    </property>
</configuration>
```

### 编辑 slaves（DN）

```shell
[root@node01 hadoop]# vi slaves # slaves 规划了DataNode 在哪启动

把 localhost改成 node01
```

### namenode格式化

```shell
[root@node01 hadoop]# hdfs namenode -format
当看到 20/06/11 12:19:14 INFO common.Storage: Storage directory /var/data/hadoop/local/dfs/name has been successfully formatted.
说明格式化成功
```

格式化成功之后在 /var/data/hadoop/local/dfs/ 会有一个name目录

```shell
[root@node01 dfs]# pwd
/var/data/hadoop/local/dfs
[root@node01 dfs]# ll
total 0
drwxr-xr-x 3 root root 40 Jan 15 07:34 name # 格式化产生，保存初始化文件
[root@node01 dfs]# 
```

### 启动 dfs

```shell
[root@node01 current]# start-dfs.sh 
Starting namenodes on [node01]
node01: starting namenode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-namenode-node01.out
node01: starting datanode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-datanode-node01.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-secondarynamenode-node01.out
```

启动成功后观察  /var/data/hadoop/local/dfs/

```shell
[root@node01 dfs]# pwd
/var/data/hadoop/local/dfs
[root@node01 dfs]# ll
total 0
drwx------ 3 root root 40 Jan 15 07:34 data # 如果是分布式，会出现在对应的机器上
drwxr-xr-x 3 root root 40 Jan 15 07:34 name
drwxr-xr-x 3 root root 40 Jan 15 07:35 namesecondary # 如果是分布式，会出现在对应的机器上
[root@node01 dfs]# 
```

### 查看进程

```shell
[root@node01 current]# jps
21520 Jps
21335 SecondaryNameNode
21162 DataNode
21067 NameNode
```

### 网页访问

http://node01:50070/dfshealth.html#tab-overview

### 停止dfs

```shell
[root@node01 current]# stop-dfs.sh 
Stopping namenodes on [node01]
node01: stopping namenode
node01: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
```

## hdfs文件操作命令

- 新建目录

  ```shell
  [root@node01 ~]# hdfs dfs -mkdir -p /user/root
  ```

- 上传文件

  ```shell
  [root@node01 ~]# hdfs dfs -put hadoop-2.9.2.tar.gz  /user/root
  ```

- 指定上传文件的块大小

  ```shell
  [root@node01 ~]# hdfs dfs -D dfs.blocksize=1048576 -put tengine-2.1.0.tar.gz
  ```

- 文件在hdfs上的存储路径

  ```shell
  [root@node01 subdir0]# pwd
  /var/data/hadoop/local/dfs/data/current/BP-2022337354-192.168.235.11-1591892354334/current/finalized/subdir0/subdir0
  [root@node01 subdir0]# ll
  total 362300
  -rw-r--r-- 1 root root 134217728 Jun 11 12:33 blk_1073741825
  -rw-r--r-- 1 root root   1048583 Jun 11 12:33 blk_1073741825_1001.meta
  -rw-r--r-- 1 root root 134217728 Jun 11 12:33 blk_1073741826
  -rw-r--r-- 1 root root   1048583 Jun 11 12:33 blk_1073741826_1002.meta
  -rw-r--r-- 1 root root  98011993 Jun 11 12:33 blk_1073741827
  -rw-r--r-- 1 root root    765727 Jun 11 12:33 blk_1073741827_1003.meta
  -rw-r--r-- 1 root root   1048576 Jun 11 12:40 blk_1073741828
  -rw-r--r-- 1 root root      8199 Jun 11 12:40 blk_1073741828_1004.meta
  -rw-r--r-- 1 root root    604664 Jun 11 12:40 blk_1073741829
  -rw-r--r-- 1 root root      4731 Jun 11 12:40 blk_1073741829_1005.meta
  [root@node01 subdir0]# 
  ```

**hdfs 相关参数配置地址**

`https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml`





